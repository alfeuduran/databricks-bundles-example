# DABs CI/CD Pipeline Example on AWS

## Introduction

This example demonstrates the setup of a CI/CD pipeline for managing Databricks Personal Access Tokens (PATs) for Development and Production workspaces using AWS Secrets Manager. It is essential to configure the tokens and workspace URLs correctly before launching the CI/CD process. For detailed guidance on Databricks PATs, please refer to the [Databricks PAT documentation](https://docs.databricks.com/en/dev-tools/auth/pat.html).


## Running Locally

1. Install the Databricks CLI from [Databricks CLI Documentation](https://docs.databricks.com/dev-tools/cli/databricks-cli.html)
    The `bundlesDevOpsDemo` project was generated by using the default-python template.
    [Databricks Templates Documentation](https://docs.databricks.com/en/dev-tools/bundles/templates.html)
    ```
    $ databricks bundle init
    ```

2. Authenticate to your Databricks workspace, if you have not done so already:
    ```
    $ databricks configure
    ```

3. Install Python dependencies:
    ```
    $ pip install -r requirements.dev.txt
    ```

4. To deploy a development copy of this project, type:
    ```
    $ databricks bundle deploy --target dev
    ```
    (Note that "dev" is the default target, so the `--target` parameter
    is optional here.)

    This deploys everything that's defined for this project.
    For example, the default template would deploy a job called
    `[dev yourname] bundlesDevOpsDemo_job` to your workspace.
    You can find that job by opening your workspace and clicking on **Workflows**.

5. Similarly, to deploy a production copy, type:
    ```
    $ databricks bundle deploy --target prod
    ```

    Note that the default job from the template has a schedule that runs every day
    (defined in `resources/bundlesDevOpsDemo_job.yml`). The schedule
    is paused when deploying in development mode (see
    [Deployment Modes Documentation](https://docs.databricks.com/dev-tools/bundles/deployment-modes.html)).

6. To run a job or pipeline, use the "run" command:
    ```
    $ databricks bundle run
    ```

7. Optionally, install developer tools such as the Databricks extension for Visual Studio Code from
    [VS Code Extension Documentation](https://docs.databricks.com/dev-tools/vscode-ext.html). Or read the "getting started" documentation for
    **Databricks Connect** for instructions on running the included Python code from a different IDE.

8. For documentation on the Databricks asset bundles format used
    for this project, and for CI/CD configuration, see
    [Databricks Asset Bundles Documentation](https://docs.databricks.com/dev-tools/bundles/index.html).

## Deploy to AWS CodePipeline

The AWS CodePipeline that will be configured will follow the following flow:

![Development workflow](images/cicd-workflow.png)

### Prerequisites

The prerequisites for proceeding with the creation of the CI/CD process within AWS are as follows:

1. Create a new project:
    ![](images/1-devOpsProjectCreate.png)

2. Choose where your code is, in this case, the code is in a repository on GitHub:
    ![](images/2-repoCreation.png)

3. Since there is already an `azure-pipelines.yml` file in the repository, it will be automatically selected for the creation of the pipeline in AWS CodePipeline.

4. Configure pipeline variables:
    - Variables can be configured in AWS CodePipeline -> **Pipeline Settings**.
    - Refer to [AWS CodePipeline Documentation](https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-variables.html) for more details.

    * `DATABRICKS_HOST` (e.g., https://xxx.azuredatabricks.net/)
    * `DATABRICKS_TOKEN` (Refer to [Databricks PAT Token Documentation](https://docs.databricks.com/en/dev-tools/auth/pat.html))
    * `env` (dev/prod)

5. The pipeline will be triggered for every pull request within the specified branch.

6. Once code is pushed to the specified branch, a new pipeline is started:
    ![Pipeline start](images/6-pipeline.png)

### Pipeline Configuration

The provided CloudFormation template creates a sample AWS CodePipeline for the Databricks Asset Bundles (DABs) solution.

```yaml
xxxxxxxx
```

#### Resources Created

- **S3 Bucket for Artifact Storage**
- **IAM Roles and Policies for CodePipeline and CodeBuild**
- **CodeCommit Repository and Event Rules**
- **CodePipeline Stages**:
  - **Source**: Fetches code from CodeCommit.
  - **Staging**: Deploys to Databricks Dev and QA environments.
  - **Production**: Deploys to Databricks Prod environment after manual approval.

Refer to the provided CloudFormation template for detailed configuration.

### Artifacts

Upon completion, the pipeline artifacts are available in the configured Databricks workspace (`databricks.yml`).

![pipeline run](images/7-pipelineRun.png)

For detailed information on project configurations and CI/CD integrations, refer to the [Databricks asset bundles documentation](https://docs.databricks.com/en/dev-tools/bundles/index.html).
